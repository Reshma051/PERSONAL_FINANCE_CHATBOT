{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok transformers accelerate torch\n"
      ],
      "metadata": {
        "id": "C9GBM-1REKhL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTHTOKEN = \"31EF4pPCBK135AfzktlSmABX4IN_35kdCYYL1rMJy4b8ccN3x\"  # paste your token here\n",
        "\n",
        "if NGROK_AUTHTOKEN:\n",
        "    from pyngrok import conf\n",
        "    conf.get_default().auth_token = NGROK_AUTHTOKEN\n",
        "    print(\"ngrok token set ‚úÖ\")\n",
        "else:\n",
        "    print(\"No token set ‚Äî if you see 401 errors later, rerun with token.\")\n"
      ],
      "metadata": {
        "id": "qOpGzRKwEKlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7c7ac6-4719-4d60-b6b6-7d01bbe961cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok token set ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os, json, requests, torch\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "st.set_page_config(page_title=\"üí∞ Personal Finance Chatbot\", layout=\"wide\")\n",
        "st.title(\"üí∞ Personal Finance Chatbot\")\n",
        "st.caption(\"Intelligent guidance for savings, taxes, and investments.\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_granite():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-instruct\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"ibm-granite/granite-3.3-2b-instruct\",\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=None\n",
        "    ).to(device)\n",
        "    return tokenizer, model, device\n",
        "\n",
        "tokenizer, model, device = load_granite()\n",
        "\n",
        "def generate_response(prompt, target_points=6, chunk_size=250):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    output_text = \"\"\n",
        "    current_points = 0\n",
        "\n",
        "    while current_points < target_points:\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=chunk_size,\n",
        "                eos_token_id=None,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        new_text = tokenizer.decode(\n",
        "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        output_text += new_text\n",
        "        current_points = output_text.count(\"\\n\")  # crude way: count list points\n",
        "        inputs = tokenizer(output_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        if current_points >= target_points:\n",
        "            break\n",
        "\n",
        "    return output_text.strip()\n",
        "\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import torch\n",
        "import re\n",
        "\n",
        "class SentenceEndCriteria(StoppingCriteria):\n",
        "    def _init_(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def _call_(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        # Stop only if punctuation is NOT part of a numbered list\n",
        "        if re.search(r\"(?<!\\d)[.?!](\\s|\\n)$\", text):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=800):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=tokenizer.eos_token_id,  # allow proper stop\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    text = tokenizer.decode(\n",
        "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # üõ† Ensure we don‚Äôt return a half-written last point\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip() and not line.strip().endswith((\".\", \"!\", \"?\")):\n",
        "            break  # stop if a line ends without punctuation\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_hf_sentiment():\n",
        "    return pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    clf = load_hf_sentiment()\n",
        "    res = clf(text)[0]\n",
        "    return {\"label\": res[\"label\"], \"score\": float(res[\"score\"])}\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"ü§ñ Q&A\", \"üìä Budget Summary\", \"üí° Spending Insights\", \"üîç NLU Analysis\"])\n",
        "\n",
        "with tab1:\n",
        "    persona = st.selectbox(\"Persona\", [\"student\", \"professional\", \"general\"])\n",
        "    question = st.text_area(\"Your Question\")\n",
        "    if st.button(\"Get Advice\"):\n",
        "        st.write(generate_response(f\"You are a helpful financial advisor for a {persona}. {question}\"))\n",
        "\n",
        "with tab2:\n",
        "    income = st.number_input(\"Monthly Income\", min_value=0.0, step=100.0)\n",
        "    expenses = st.text_area(\"Expenses JSON\", '{\"rent\": 1000, \"food\": 300}')\n",
        "    goal = st.number_input(\"Savings Goal\", min_value=0.0, step=50.0)\n",
        "    if st.button(\"Analyze Budget\"):\n",
        "        ex = json.loads(expenses)\n",
        "        total_exp = sum(ex.values())\n",
        "        surplus = income - total_exp\n",
        "        progress = (surplus / goal * 100) if goal else None\n",
        "        st.json({\"income\": income, \"total_expenses\": total_exp, \"surplus\": surplus, \"goal_progress_percent\": progress})\n",
        "\n",
        "with tab3:\n",
        "    income2 = st.number_input(\"Monthly Income\", min_value=0.0, step=100.0, key=\"si_income\")\n",
        "    expenses2 = st.text_area(\"Expenses JSON\", '{\"rent\": 1000, \"food\": 300}', key=\"si_exp\")\n",
        "    goals2 = st.text_area(\"Goals JSON\", '[{\"name\":\"Laptop\",\"target_amount\":1200,\"target_months\":6}]')\n",
        "    if st.button(\"Get Insights\"):\n",
        "        ex = json.loads(expenses2)\n",
        "        goals_list = json.loads(goals2)\n",
        "        surplus = income2 - sum(ex.values())\n",
        "        insights = []\n",
        "        for g in goals_list:\n",
        "            monthly_needed = g[\"target_amount\"] / g[\"target_months\"]\n",
        "            insights.append({\"goal\": g[\"name\"], \"achievable\": surplus >= monthly_needed})\n",
        "        st.json(insights)\n",
        "\n",
        "with tab4:\n",
        "    text_nlu = st.text_area(\"Enter text for sentiment analysis\")\n",
        "    if st.button(\"Analyze Sentiment\"):\n",
        "        st.json(analyze_sentiment(text_nlu))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqP6UPwgNdDe",
        "outputId": "75856ae2-011e-44d2-8460-0660ce0f9e72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading, os, time\n",
        "\n",
        "ngrok.kill()\n",
        "PORT = 8501\n",
        "public_url = ngrok.connect(PORT)\n",
        "print(\"üåê Streamlit UI:\", public_url)\n",
        "\n",
        "def run():\n",
        "    os.system(f\"streamlit run app.py --server.port {PORT}\")\n",
        "\n",
        "threading.Thread(target=run, daemon=True).start()\n",
        "time.sleep(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLeW0fdNNiQf",
        "outputId": "5fff33c0-d36b-471a-99a0-d37c4a5da433"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Streamlit UI: NgrokTunnel: \"https://a071784f8931.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}